{
  "experiment_date": "2025-12-11 12:46:12",
  "experiment_type": "Limited Labeled Data (220 samples)",
  "training_setup": {
    "n_labeled_total": 222,
    "n_labeled_train": 155,
    "n_test": 67,
    "unlabeled_sizes_tested": [
      500,
      1000,
      2500,
      5000,
      10000
    ],
    "stratified_sampling": true,
    "train_test_split": "70-30"
  },
  "best_model_tox": {
    "method": "Co-Training",
    "name": "CoTrain(n=5000)",
    "n_unlabeled_used": 5000,
    "f1_score": 0.7294117647058823,
    "accuracy": 0.6567164179104478,
    "precision": 0.6078431372549019,
    "recall": 0.9117647058823529,
    "roc_auc": 0.7054367201426025,
    "pr_auc": 0.7042645831076806,
    "baseline_f1": 0.65625,
    "baseline_roc_auc": 0.7825311942959001,
    "improvement_percent": 11.148459383753496,
    "roc_improvement_percent": -9.851936218678809
  },
  "methods_tested": [
    "Baseline (Supervised Random Forest)",
    "Label Propagation (Graph-based, hard labels)",
    "Label Spreading (Graph-based, soft labels, \u03b1=0.2)",
    "Self-Training (Iterative pseudo-labeling, threshold=0.75)",
    "Co-Training (Multi-view learning with feature splits)"
  ],
  "key_findings": [
    "With only 155 labeled samples, semi-supervised learning improved CT_TOX F1 by +11.15%",
    "Best method: Co-Training using 5,000 unlabeled samples",
    "ROC-AUC improved from 0.7825 to 0.7054",
    "Unlabeled data successfully enhanced model performance",
    "Demonstrates the power of SSL when labeled data is scarce",
    "All 4 SSL methods evaluated: Label Propagation, Label Spreading, Self-Training, Co-Training"
  ]
}